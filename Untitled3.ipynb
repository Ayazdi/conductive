{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'contrib'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-bc0f7fffde6b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m   \u001b[1;31m# to learn about it.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m   \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mNormal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNormal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[0mBernoulli\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistributions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBernoulli\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'contrib'"
     ]
    }
   ],
   "source": [
    "# https://deeplearningcourses.com/c/deep-learning-gans-and-variational-autoencoders\n",
    "# https://www.udemy.com/deep-learning-gans-and-variational-autoencoders\n",
    "from __future__ import print_function, division\n",
    "from builtins import range, input\n",
    "# Note: you may need to update your version of future\n",
    "# sudo pip install -U future\n",
    "\n",
    "# import util\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "st = None\n",
    "try:\n",
    "  st = tf.contrib.bayesflow.stochastic_tensor\n",
    "except:\n",
    "  # doesn't exist in later versions of TF\n",
    "  # we will use the reparameterization trick instead\n",
    "  # watch the later lecture on the reparameterization trick\n",
    "  # to learn about it.\n",
    "  pass\n",
    "Normal = tf.contrib.distributions.Normal\n",
    "Bernoulli = tf.contrib.distributions.Bernoulli\n",
    "\n",
    "\n",
    "\n",
    "class DenseLayer(object):\n",
    "  def __init__(self, M1, M2, f=tf.nn.relu):\n",
    "    # self.M1 = M1\n",
    "    # self.M2 = M2\n",
    "\n",
    "    self.W = tf.Variable(tf.random_normal(shape=(M1, M2)) * 2 / np.sqrt(M1))\n",
    "    self.b = tf.Variable(np.zeros(M2).astype(np.float32))\n",
    "    self.f = f\n",
    "\n",
    "  def forward(self, X):\n",
    "    return self.f(tf.matmul(X, self.W) + self.b)\n",
    "\n",
    "\n",
    "class VariationalAutoencoder:\n",
    "  def __init__(self, D, hidden_layer_sizes):\n",
    "    # hidden_layer_sizes specifies the size of every layer\n",
    "    # in the encoder\n",
    "    # up to the final hidden layer Z\n",
    "    # the decoder will have the reverse shape\n",
    "\n",
    "    \n",
    "    # represents a batch of training data\n",
    "    self.X = tf.placeholder(tf.float32, shape=(None, D))\n",
    "\n",
    "    # encoder\n",
    "    self.encoder_layers = []\n",
    "    M_in = D\n",
    "    for M_out in hidden_layer_sizes[:-1]:\n",
    "      h = DenseLayer(M_in, M_out)\n",
    "      self.encoder_layers.append(h)\n",
    "      M_in = M_out\n",
    "\n",
    "\n",
    "    # for convenience, we'll refer to the final encoder size as M\n",
    "    # also the input to the decoder size\n",
    "    M = hidden_layer_sizes[-1]\n",
    "\n",
    "    # the encoder's final layer output is unbounded\n",
    "    # so there is no activation function\n",
    "    # we also need 2 times as many units as specified by M_out\n",
    "    # since there needs to be M_out means + M_out variances\n",
    "    h = DenseLayer(M_in, 2 * M, f=lambda x: x)\n",
    "    self.encoder_layers.append(h)\n",
    "\n",
    "    # get the mean and variance / std dev of Z.\n",
    "    # note that the variance must be > 0\n",
    "    # we can get a sigma (standard dev) > 0 from an unbounded variable by\n",
    "    # passing it through the softplus function.\n",
    "    # add a small amount for smoothing.\n",
    "    current_layer_value = self.X\n",
    "    for layer in self.encoder_layers:\n",
    "      current_layer_value = layer.forward(current_layer_value)\n",
    "    self.means = current_layer_value[:, :M]\n",
    "    self.stddev = tf.nn.softplus(current_layer_value[:, M:]) + 1e-6\n",
    "\n",
    "    # get a sample of Z\n",
    "    # we need to use a stochastic tensor\n",
    "    # in order for the errors to be backpropagated past this point\n",
    "    if st is None:\n",
    "      # doesn't exist in later versions of Tensorflow\n",
    "      # we'll use the same trick we use in Theano\n",
    "      standard_normal = Normal(\n",
    "        loc=np.zeros(M, dtype=np.float32),\n",
    "        scale=np.ones(M, dtype=np.float32)\n",
    "      )\n",
    "      e = standard_normal.sample(tf.shape(self.means)[0])\n",
    "      self.Z = e * self.stddev + self.means\n",
    "\n",
    "      # note: this also works because Tensorflow\n",
    "      # now does the \"magic\" for you\n",
    "      # n = Normal(\n",
    "      #   loc=self.means,\n",
    "      #   scale=self.stddev,\n",
    "      # )\n",
    "      # self.Z = n.sample()\n",
    "    else:\n",
    "      with st.value_type(st.SampleValue()):\n",
    "        self.Z = st.StochasticTensor(Normal(loc=self.means, scale=self.stddev))\n",
    "        # to get back Q(Z), the distribution of Z\n",
    "        # we will later use self.Z.distribution\n",
    "\n",
    "\n",
    "    # decoder\n",
    "    self.decoder_layers = []\n",
    "    M_in = M\n",
    "    for M_out in reversed(hidden_layer_sizes[:-1]):\n",
    "      h = DenseLayer(M_in, M_out)\n",
    "      self.decoder_layers.append(h)\n",
    "      M_in = M_out\n",
    "\n",
    "    # the decoder's final layer should technically go through a sigmoid\n",
    "    # so that the final output is a binary probability (e.g. Bernoulli)\n",
    "    # but Bernoulli accepts logits (pre-sigmoid) so we will take those\n",
    "    # so no activation function is needed at the final layer\n",
    "    h = DenseLayer(M_in, D, f=lambda x: x)\n",
    "    self.decoder_layers.append(h)\n",
    "\n",
    "    # get the logits\n",
    "    current_layer_value = self.Z\n",
    "    for layer in self.decoder_layers:\n",
    "      current_layer_value = layer.forward(current_layer_value)\n",
    "    logits = current_layer_value\n",
    "    posterior_predictive_logits = logits # save for later\n",
    "\n",
    "    # get the output\n",
    "    self.X_hat_distribution = Bernoulli(logits=logits)\n",
    "\n",
    "    # take samples from X_hat\n",
    "    # we will call this the posterior predictive sample\n",
    "    self.posterior_predictive = self.X_hat_distribution.sample()\n",
    "    self.posterior_predictive_probs = tf.nn.sigmoid(logits)\n",
    "\n",
    "    # take sample from a Z ~ N(0, 1)\n",
    "    # and put it through the decoder\n",
    "    # we will call this the prior predictive sample\n",
    "    standard_normal = Normal(\n",
    "      loc=np.zeros(M, dtype=np.float32),\n",
    "      scale=np.ones(M, dtype=np.float32)\n",
    "    )\n",
    "\n",
    "    Z_std = standard_normal.sample(1)\n",
    "    current_layer_value = Z_std\n",
    "    for layer in self.decoder_layers:\n",
    "      current_layer_value = layer.forward(current_layer_value)\n",
    "    logits = current_layer_value\n",
    "\n",
    "    prior_predictive_dist = Bernoulli(logits=logits)\n",
    "    self.prior_predictive = prior_predictive_dist.sample()\n",
    "    self.prior_predictive_probs = tf.nn.sigmoid(logits)\n",
    "\n",
    "\n",
    "    # prior predictive from input\n",
    "    # only used for generating visualization\n",
    "    self.Z_input = tf.placeholder(tf.float32, shape=(None, M))\n",
    "    current_layer_value = self.Z_input\n",
    "    for layer in self.decoder_layers:\n",
    "      current_layer_value = layer.forward(current_layer_value)\n",
    "    logits = current_layer_value\n",
    "    self.prior_predictive_from_input_probs = tf.nn.sigmoid(logits)\n",
    "\n",
    "\n",
    "    # now build the cost\n",
    "    if st is None:\n",
    "      kl = -tf.log(self.stddev) + 0.5*(self.stddev**2 + self.means**2) - 0.5\n",
    "      kl = tf.reduce_sum(kl, axis=1)\n",
    "    else:\n",
    "      kl = tf.reduce_sum(\n",
    "        tf.contrib.distributions.kl_divergence(\n",
    "          self.Z.distribution, standard_normal\n",
    "        ),\n",
    "        1\n",
    "      )\n",
    "    expected_log_likelihood = tf.reduce_sum(\n",
    "      self.X_hat_distribution.log_prob(self.X),\n",
    "      1\n",
    "    )\n",
    "\n",
    "    # equivalent\n",
    "    # expected_log_likelihood = -tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "    #   labels=self.X,\n",
    "    #   logits=posterior_predictive_logits\n",
    "    # )\n",
    "    # expected_log_likelihood = tf.reduce_sum(expected_log_likelihood, 1)\n",
    "\n",
    "\n",
    "\n",
    "    self.elbo = tf.reduce_sum(expected_log_likelihood - kl)\n",
    "    self.train_op = tf.train.RMSPropOptimizer(learning_rate=0.001).minimize(-self.elbo)\n",
    "\n",
    "    # set up session and variables for later\n",
    "    self.init_op = tf.global_variables_initializer()\n",
    "    self.sess = tf.InteractiveSession()\n",
    "    self.sess.run(self.init_op)\n",
    "\n",
    "\n",
    "  def fit(self, X, epochs=30, batch_sz=64):\n",
    "    costs = []\n",
    "    n_batches = len(X) // batch_sz\n",
    "    print(\"n_batches:\", n_batches)\n",
    "    for i in range(epochs):\n",
    "      print(\"epoch:\", i)\n",
    "      np.random.shuffle(X)\n",
    "      for j in range(n_batches):\n",
    "        batch = X[j*batch_sz:(j+1)*batch_sz]\n",
    "        _, c, = self.sess.run((self.train_op, self.elbo), feed_dict={self.X: batch})\n",
    "        c /= batch_sz # just debugging\n",
    "        costs.append(c)\n",
    "        if j % 100 == 0:\n",
    "          print(\"iter: %d, cost: %.3f\" % (j, c))\n",
    "    plt.plot(costs)\n",
    "    plt.show()\n",
    "\n",
    "  def transform(self, X):\n",
    "    return self.sess.run(\n",
    "      self.means,\n",
    "      feed_dict={self.X: X}\n",
    "    )\n",
    "\n",
    "  def prior_predictive_with_input(self, Z):\n",
    "    return self.sess.run(\n",
    "      self.prior_predictive_from_input_probs,\n",
    "      feed_dict={self.Z_input: Z}\n",
    "    )\n",
    "\n",
    "  def posterior_predictive_sample(self, X):\n",
    "    # returns a sample from p(x_new | X)\n",
    "    return self.sess.run(self.posterior_predictive, feed_dict={self.X: X})\n",
    "\n",
    "  def prior_predictive_sample_with_probs(self):\n",
    "    # returns a sample from p(x_new | z), z ~ N(0, 1)\n",
    "    return self.sess.run((self.prior_predictive, self.prior_predictive_probs))\n",
    "\n",
    "\n",
    "def main():\n",
    "  X, Y = util.get_mnist()\n",
    "  # convert X to binary variable\n",
    "  X = (X > 0.5).astype(np.float32)\n",
    "\n",
    "  vae = VariationalAutoencoder(784, [200, 100])\n",
    "  vae.fit(X)\n",
    "\n",
    "  # plot reconstruction\n",
    "  done = False\n",
    "  while not done:\n",
    "    i = np.random.choice(len(X))\n",
    "    x = X[i]\n",
    "    im = vae.posterior_predictive_sample([x]).reshape(28, 28)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(x.reshape(28, 28), cmap='gray')\n",
    "    plt.title(\"Original\")\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(im, cmap='gray')\n",
    "    plt.title(\"Sampled\")\n",
    "    plt.show()\n",
    "\n",
    "    ans = input(\"Generate another?\")\n",
    "    if ans and ans[0] in ('n' or 'N'):\n",
    "      done = True\n",
    "\n",
    "  # plot output from random samples in latent space\n",
    "  done = False\n",
    "  while not done:\n",
    "    im, probs = vae.prior_predictive_sample_with_probs()\n",
    "    im = im.reshape(28, 28)\n",
    "    probs = probs.reshape(28, 28)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(im, cmap='gray')\n",
    "    plt.title(\"Prior predictive sample\")\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(probs, cmap='gray')\n",
    "    plt.title(\"Prior predictive probs\")\n",
    "    plt.show()\n",
    "\n",
    "    ans = input(\"Generate another?\")\n",
    "    if ans and ans[0] in ('n' or 'N'):\n",
    "      done = True\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting until\n",
      "  Downloading until-0.1.2-py3-none-any.whl (2.9 kB)\n",
      "Note: you may need to restart the kernel to use updated packages.Installing collected packages: until\n",
      "\n",
      "Successfully installed until-0.1.2\n"
     ]
    }
   ],
   "source": [
    "pip install until"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
